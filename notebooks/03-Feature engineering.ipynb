{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "\n",
    "Now, we will work on feature engineering to optimize our model.\n",
    "We can divide our features in two types: Meta and Text based.\n",
    "\n",
    "### Meta features\n",
    "These will include all the features that we get from the user or information we can obtain given our understanding of the english language.\n",
    "\n",
    "1. Post upvotes \n",
    "2. Days since request\n",
    "3. User subs on (count of subreddits) \n",
    "4. User activity comments\n",
    "5. User activity comments raop \n",
    "6. User posts reddit \n",
    "7. User posts raop \n",
    "8. Comment count \n",
    "9. User rate start \n",
    "10. User rate end \n",
    "11. Account age (days)\n",
    "12. Day of the week\n",
    "13. Day of the month\n",
    "14. Month\n",
    "15. Week of the year\n",
    "16. Text was edited\n",
    "\n",
    "### Text based features\n",
    "These will include all the features that are related to frequency, svd, sentiment.\n",
    "\n",
    "1. Text length (word count) \n",
    "2. Text compound sentiment \n",
    "3. Title length (word count)\n",
    "4. Title sentiment\n",
    "5. TFIDF\n",
    "6. SVD\n",
    "\n",
    "## Let's import libraries and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# Data manipulation\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ML Scikit\n",
    "\n",
    "# NLP\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "\n",
    "## SVD\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "## Decision Trees\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "#import xgboost as xgb\n",
    "\n",
    "# Regression model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# KNN Model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Feature handling\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Reports\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "\n",
    "# For producing decision tree diagrams.\n",
    "from IPython.core.display import Image, display\n",
    "from six import StringIO\n",
    "\n",
    "#Visualizing Confusion Matrix using Heatmap\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "df = pd.read_csv('../data/interim/logit_sentiments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0., ...,  1.,  1., nan])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(list(df['requester_received_pizza']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['request_text', 'request_title', 'post_upvotes', 'text_word_count',\n",
       "       'text_sentiment', 'title_word_count', 'title_sentiment',\n",
       "       'days_since_request', 'user_activity_comments',\n",
       "       'user_activity_comments_raop', 'user_posts_reddit', 'user_posts_raop',\n",
       "       'user_rate_start', 'user_rate_end', 'day_request', 'day_month_request',\n",
       "       'month_request', 'week_request', 'requester_received_pizza'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data to evaluate new features\n",
    "\n",
    "We're using the baseline classifier - Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Handling for Model\n",
    "\n",
    "X, Y = df.iloc[:,:-1], df.iloc[:,-1]\n",
    "\n",
    "# Shuffle the data, but make sure that the features and accompanying labels stay in sync.\n",
    "np.random.seed(0)\n",
    "shuffle = np.random.permutation(np.arange(X.shape[0]))\n",
    "X, Y = X.iloc[shuffle], Y.iloc[shuffle]\n",
    "\n",
    "# Split into train and test.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=1)\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(X_test, y_test, test_size=0.4, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train X,y -  (3030, 18)   (3030,)\n",
      "Dev X,y   -   (606, 18)    (606,)\n",
      "Test X,y  -   (405, 18)    (405,)\n",
      "Total:  4041\n"
     ]
    }
   ],
   "source": [
    "# Let's see the different tests\n",
    "\n",
    "print(\"Train X,y - \", X_train.shape, \" \", y_train.shape)\n",
    "print(\"Dev X,y   -  \", X_dev.shape, \"  \", y_dev.shape)\n",
    "print(\"Test X,y  -  \", X_test.shape, \"  \", y_test.shape)\n",
    "print(\"Total: \", X_train.shape[0] + X_dev.shape[0] +  X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing text with TFIDF vectorizer and SVD dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see first the text we are interested in analyzing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check nan\n",
    "# pd.isnull(X_train['request_text']).sum()\n",
    "# X_train['request_text'].count()\n",
    "\n",
    "# See text messages\n",
    "X_train.loc[:,'request_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fills nan in text features\n",
    "# Input: array, array name\n",
    "# Output: fill array nan with 'the'\n",
    "def fill_nan_text(texts, feature):\n",
    "    texts = texts.replace(np.nan, 'the', regex=True)\n",
    "    return texts\n",
    "\n",
    "# Improve text tokenization\n",
    "# Input: string\n",
    "# Output: string without stop words, and applied\n",
    "# lemmatization and stemming\n",
    "def better_preprocessor(s):\n",
    "    \n",
    "    s = s.lower() # to lowercase\n",
    "    \n",
    "    #Stemming and lemmatization\n",
    "    ps = PorterStemmer()\n",
    "    lemmer = WordNetLemmatizer()\n",
    "    words = word_tokenize(s)\n",
    "    cleaned_words = [lemmer.lemmatize(ps.stem(word)) for word in words]\n",
    "    \n",
    "    #Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    new_cleaned_words = [w for w in cleaned_words if not w.lower() in stop_words]\n",
    "    s = ''\n",
    "    for word in new_cleaned_words:\n",
    "        if word == '.' or word == '?' or word == ',':\n",
    "            s = s + word\n",
    "        else:\n",
    "            s = s + ' ' + word\n",
    "    \n",
    "    return s\n",
    "\n",
    "# Vectorizes the text data into separated tokens as features\n",
    "# Input: array with text\n",
    "# Output: print size of vocabulary\n",
    "def create_text_features_no_processor(train):\n",
    "    \n",
    "    # Vectorization with empty preprocessing of text\n",
    "    vectorizer = CountVectorizer(preprocessor= lambda x: x)\n",
    "    X_train_counts_raw = vectorizer.fit_transform(train)\n",
    "    print('Size of train Vocabulary with empty pre-processor is:', X_train_counts_raw.shape)\n",
    "\n",
    "# Vectorizes the text data into separated tokens as features and applies better processor\n",
    "# Input: array with text\n",
    "# Output: print size of vocabulary\n",
    "def create_text_features_with_processor(train, test):\n",
    "    \n",
    "    # Vectorization with better preprocessor\n",
    "    # declare Vectorizer with counts of words\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "                                       analyzer='word',\n",
    "                                       token_pattern=r'\\w{1,}',\n",
    "#                                        min_df=3, \n",
    "                                       preprocessor = better_preprocessor,\n",
    "                                       ngram_range=(1,3))\n",
    "\n",
    "    #vectorizer on train data\n",
    "    X_train_counts = tfidf_vectorizer.fit_transform(fill_nan_text(train,'request_text'))\n",
    "    X_test = tfidf_vectorizer.transform(fill_nan_text(test,'request_text'))\n",
    "    print('---------- \\nSize of train Vocabulary with better preprocessor is:', X_train_counts.shape[1])\n",
    "    \n",
    "    #feature names\n",
    "    feature_names = tfidf_vectorizer.get_feature_names()\n",
    "#     print(len(feature_names))\n",
    "    return tfidf_vectorizer,X_train_counts, X_test, feature_names\n",
    "\n",
    "def svd_reduction(X_train_counts,X_test,svd_comps, feature_names):\n",
    "    \n",
    "    #SVD for dimensionality reduction\n",
    "    svd = TruncatedSVD(n_components=svd_comps,n_iter=10,random_state=42)\n",
    "    normalizer = Normalizer(copy=False) #Normalizing SVD results\n",
    "    lsa = make_pipeline(svd, normalizer) #making pipeline for svd and normalizer\n",
    "\n",
    "    ## reduced by SVD\n",
    "    X_svd = lsa.fit_transform(X_train_counts)\n",
    "    X_test_svd = lsa.transform(X_test)\n",
    "    print('---------- \\nSize of train Vocabulary with better preprocessor and SVD is:', X_svd.shape[1])\n",
    "    \n",
    "    #explained variance by SVD reduced dimensions\n",
    "    explained_variance = svd.explained_variance_ratio_.sum()\n",
    "    print(\"Explained variance of the SVD step: {}%\".format(int(explained_variance * 100)))\n",
    "    \n",
    "    #extract feature names corresponding to the 1500 SVD components\n",
    "    best_features = [feature_names[i] for i in svd.components_[0].argsort()[::-1]]\n",
    "    #print(best_features[:1500])\n",
    "    \n",
    "    print(f\"Train array: {X_svd.shape}\\nTest array: {X_test_svd.shape}\")\n",
    "    \n",
    "    #Convert to pandas df\n",
    "#     df_orig = pd.DataFrame(X_train_counts.toarray(), columns=feature_names)\n",
    "#     df = pd.DataFrame(df_orig, columns=best_features[:1500])\n",
    "  \n",
    "    return X_svd, X_test_svd\n",
    "\n",
    "def clean_array(array):\n",
    "    # Remove original text variables\n",
    "    array.pop('request_title')\n",
    "    array.pop('request_text')\n",
    "    # X_dev.columns\n",
    "    return array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge meta and text data for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- \n",
      "Size of train Vocabulary with better preprocessor is: 199989\n",
      "---------- \n",
      "Size of train Vocabulary with better preprocessor and SVD is: 200\n",
      "Explained variance of the SVD step: 13%\n",
      "Train array: (3030, 200)\n",
      "Test array: (606, 200)\n",
      "Train meta: (3030, 16) - Train Text SVD: (3030, 200)\n",
      "Test meta: (606, 16) - Test Text SVD: (606, 200)\n",
      "Test meta + text: (3030, 216) - Test meta + text: (606, 216)\n",
      "**************************\n",
      "Logistic regression\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: 'unknown'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-e17546c0429a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m lr_model = LogisticRegression(C=cs, max_iter= 1000, solver='liblinear',\n\u001b[1;32m     48\u001b[0m                              multi_class=\"auto\", penalty ='l2')\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mlr_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1343\u001b[0m                                    \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1344\u001b[0m                                    accept_large_sparse=solver != 'liblinear')\n\u001b[0;32m-> 1345\u001b[0;31m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36mcheck_classification_targets\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    170\u001b[0m     if y_type not in ['binary', 'multiclass', 'multiclass-multioutput',\n\u001b[1;32m    171\u001b[0m                       'multilabel-indicator', 'multilabel-sequences']:\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown label type: %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown label type: 'unknown'"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "\n",
    "svd_components = 200 # tested with 200, 400, 600, 1000, 1500\n",
    "cs=1 # tested with 0.01,0.1,1,10,100\n",
    "depths = [2,3,4]\n",
    "estimators = [15, 30, 80]\n",
    "best=[0,0,0,0]\n",
    "\n",
    "# Data Handling for Model\n",
    "\n",
    "X, Y = df.iloc[:,:-1], df.iloc[:,-1]\n",
    "\n",
    "# Shuffle the data, but make sure that the features and accompanying labels stay in sync.\n",
    "np.random.seed(0)\n",
    "shuffle = np.random.permutation(np.arange(X.shape[0]))\n",
    "X, Y = X.iloc[shuffle], Y.iloc[shuffle]\n",
    "\n",
    "# Split into train and test.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=1)\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(X_test, y_test, test_size=0.4, random_state=1)\n",
    "\n",
    "\n",
    "# Vectorize with Tfidf + SVD \n",
    "# Vectorize with TfIDF + SVD\n",
    "count_vector, train_counts, test_counts, features = create_text_features_with_processor(\n",
    "    X_train.loc[:,'request_text'], \n",
    "    X_dev.loc[:,'request_text'])\n",
    "\n",
    "train_svd, test_svd = svd_reduction(train_counts, test_counts,\n",
    "                                    svd_components,\n",
    "                                   features)\n",
    "\n",
    "# Clean text variables: title and request\n",
    "X_train = clean_array(X_train)\n",
    "X_dev = clean_array(X_dev)\n",
    "\n",
    "print(f\"Train meta: {X_train.shape} - Train Text SVD: {train_svd.shape}\") \n",
    "print(f\"Test meta: {X_dev.shape} - Test Text SVD: {test_svd.shape}\")\n",
    "\n",
    "X_train_mt = np.concatenate((X_train,train_svd), axis=1)\n",
    "X_dev_mt = np.concatenate((X_dev,test_svd), axis=1)\n",
    "print(f\"Test meta + text: {X_train_mt.shape} - Test meta + text: {X_dev_mt.shape}\")\n",
    "\n",
    "# fit model with L2 Regularization\n",
    "print(\"**************************\")\n",
    "print(\"Logistic regression\")\n",
    "lr_model = LogisticRegression(C=cs, max_iter= 1000, solver='liblinear',\n",
    "                             multi_class=\"auto\", penalty ='l2')\n",
    "lr_model.fit(X_train_mt, y_train)\n",
    "\n",
    "# predictions\n",
    "y_pred = lr_model.predict(X_dev_mt)\n",
    "\n",
    "# results\n",
    "#     cnf_matrix = metrics.confusion_matrix(y_dev, y_pred)\n",
    "# print(cnf_matrix)\n",
    "#     print(classification_report(y_dev, y_pred))\n",
    "report_log = classification_report(y_dev, y_pred, output_dict=True)\n",
    "print(\"Accuracy for Logistic regression model is\", round(report_log['accuracy'],2))\n",
    "print(\"**************************\")\n",
    "print(\"Random forest classifier\")\n",
    "\n",
    "# for d in depths:\n",
    "#     for e in estimators:\n",
    "#         # XG Boost classifier\n",
    "#         clf = xgb.XGBClassifier(max_depth=d, \n",
    "#                                 n_estimators=e, \n",
    "# #                                 colsample_bytree=0.8,\n",
    "# #                                 subsample=0.8, \n",
    "# #                                 nthread=10, \n",
    "#                                 learning_rate=0.1,\n",
    "#                                 use_label_encoder=False,\n",
    "#                                 eval_metric='mlogloss')\n",
    "#         clf.fit(X_train_mt, y_train)\n",
    "#         predictions = clf.predict_proba(X_dev_mt)\n",
    "#         if clf.score(X_dev_mt, y_dev) > best[0]:\n",
    "#             print (f'Accuracy (xg boost): {clf.score(X_dev_mt, y_dev):.2} - d={d} - e={e}')\n",
    "#             best[0] = clf.score(X_dev_mt, y_dev)\n",
    "#             best[1] = d\n",
    "#             best[2] = e\n",
    "#             best[3] = 'xg_boost'\n",
    "#         # Random Forest\n",
    "#         rfc = RandomForestClassifier(n_estimators=e, max_depth=d)\n",
    "#         rfc.fit(X_train_mt, y_train)\n",
    "#         if clf.score(X_dev_mt, y_dev) > best[0]:\n",
    "#             print (f'Accuracy (RF): {rfc.score(X_dev_mt, y_dev):.2} - d={d} - e={e}')\n",
    "#             best[0] = rfc.score(X_dev_mt, y_dev)\n",
    "#             best[1] = d\n",
    "#             best[2] = e\n",
    "#             best[3] = 'RF'\n",
    "            \n",
    "#         # Ada Boost\n",
    "#         abc = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=d)\n",
    "#                                  , n_estimators=e, learning_rate=0.1)\n",
    "#         abc.fit(X_train_mt, y_train)\n",
    "#         if abc.score(X_dev_mt, y_dev) > best[0]:\n",
    "#             print(f'Accuracy (adaboost): {abc.score(X_dev_mt, y_dev):.2} - d={d} - e={e}')\n",
    "#             best[0] = rfc.score(X_dev_mt, y_dev)\n",
    "#             best[1] = d\n",
    "#             best[2] = e\n",
    "#             best[3] = 'ada'\n",
    "\n",
    "# print(\"**************************\")\n",
    "# print(f\"The best classifier is {best[3]} with acc={best[0]:.3} - d={best[1]} - e={best[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old code. We can remove in the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove original text variables\n",
    "# X_train.pop('request_title')\n",
    "# X_train.pop('request_text')\n",
    "# X_dev.pop('request_title')\n",
    "# X_dev.pop('request_text')\n",
    "# # X_dev.columns\n",
    "\n",
    "# X_train_mt = np.concatenate((X_train,train_svd), axis=1)\n",
    "# X_dev_mt = np.concatenate((X_dev,test_svd), axis=1)\n",
    "\n",
    "\n",
    "# print(f\"Test meta + text: {X_train_mt.shape} - Test meta + text: {X_dev_mt.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_mt[1,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fit model\n",
    "# lr_model = LogisticRegression(max_iter= 1000, solver='liblinear')\n",
    "# lr_model.fit(X_train_mt[:,1:], y_train)\n",
    "\n",
    "# # predictions\n",
    "# y_pred = lr_model.predict(X_dev_mt[:,1:])\n",
    "\n",
    "# # results\n",
    "# cnf_matrix = metrics.confusion_matrix(y_dev, y_pred)\n",
    "# # print(cnf_matrix)\n",
    "# print(classification_report(y_dev, y_pred))\n",
    "# report_log = classification_report(y_dev, y_pred, output_dict=True)\n",
    "# print(\"Accuracy for Logistic regression model is\", round(report_log['accuracy'],2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dt = DecisionTreeClassifier(criterion=\"entropy\", splitter=\"best\", random_state=0)\n",
    "# # dt.fit(X_train_mt[:,1:], y_train)\n",
    "\n",
    "# # print ('Accuracy (a decision tree):', dt.score(X_dev_mt[:,1:], y_dev))\n",
    "\n",
    "# rfc = RandomForestClassifier(n_estimators=20, max_depth=4)\n",
    "# rfc.fit(X_train_mt[:,1:], y_train)\n",
    "\n",
    "# print ('Accuracy (a random forest):', rfc.score(X_dev_mt[:,1:], y_dev))\n",
    "\n",
    "# abc = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), n_estimators=50, learning_rate=0.1)\n",
    "\n",
    "# abc.fit(X_train_mt[:,1:], y_train)\n",
    "# print ('Accuracy (adaboost with decision trees):', abc.score(X_dev_mt[:,1:], y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
