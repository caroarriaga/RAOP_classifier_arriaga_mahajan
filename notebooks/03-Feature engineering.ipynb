{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb9c4894-f7c1-4f0c-b20f-6d196b76f049",
   "metadata": {},
   "source": [
    "# Model Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da532d3-6d77-4883-91ea-a2d886c801fb",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "\n",
    "Now, we will work on feature engineering to optimize our model.\n",
    "We can divide our features in two types: Meta and Text based.\n",
    "\n",
    "### Meta features\n",
    "These will include all the features that we get from the user or information we can obtain given our understanding of the english language.\n",
    "\n",
    "1. Post upvotes \n",
    "2. Days since request\n",
    "3. User subs on (count of subreddits) \n",
    "4. User activity comments\n",
    "5. User activity comments raop \n",
    "6. User posts reddit \n",
    "7. User posts raop \n",
    "8. Comment count \n",
    "9. User rate start \n",
    "10. User rate end \n",
    "11. Account age (days)\n",
    "12. Day of the week\n",
    "13. Day of the month\n",
    "14. Month\n",
    "15. Week of the year\n",
    "16. Text was edited\n",
    "\n",
    "### Text based features\n",
    "These will include all the features that are related to frequency, svd, sentiment.\n",
    "\n",
    "1. Text length (word count) \n",
    "2. Text compound sentiment \n",
    "3. Title length (word count)\n",
    "4. Title sentiment\n",
    "5. TFIDF\n",
    "6. SVD\n",
    "\n",
    "## Let's import libraries and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f708a8a9-92b4-4338-8834-cefd46543923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# Data manipulation\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ML Scikit\n",
    "\n",
    "# NLP\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "\n",
    "## SVD\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "## Decision Trees\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "# Regression model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# KNN Model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Feature handling\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Reports\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "\n",
    "# For producing decision tree diagrams.\n",
    "from IPython.core.display import Image, display\n",
    "from six import StringIO\n",
    "\n",
    "# MNN and DNN\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "##Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "## Keras and TF\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras import backend as K\n",
    "\n",
    "#Visualizing Confusion Matrix using Heatmap\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5b7392b-4d49-4a56-bd13-74f3fb364552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "df = pd.read_csv('../data/interim/logit_sentiments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be65765c-40ac-42bd-8743-fe6bf2e802f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['request_text', 'request_title', 'post_upvotes', 'text_word_count',\n",
       "       'text_sentiment', 'title_word_count', 'title_sentiment',\n",
       "       'days_since_request', 'user_activity_comments',\n",
       "       'user_activity_comments_raop', 'user_posts_reddit', 'user_posts_raop',\n",
       "       'user_rate_start', 'user_rate_end', 'day_request', 'day_month_request',\n",
       "       'month_request', 'week_request', 'requester_received_pizza'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f6928a-ea5f-4bb7-9584-38f955aad630",
   "metadata": {},
   "source": [
    "# Split data to evaluate new features\n",
    "\n",
    "We're using the baseline classifier - Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "c94bd52e-bd63-4497-991d-667541b64906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Handling for Model\n",
    "\n",
    "X, Y = df.iloc[:,:-1], df.iloc[:,-1]\n",
    "\n",
    "# Shuffle the data, but make sure that the features and accompanying labels stay in sync.\n",
    "np.random.seed(0)\n",
    "shuffle = np.random.permutation(np.arange(X.shape[0]))\n",
    "X, Y = X.iloc[shuffle], Y.iloc[shuffle]\n",
    "\n",
    "# Split into train and test.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=1)\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(X_test, y_test, test_size=0.4, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "943a3c40-6371-4fb1-82cc-d5f703f3e09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train X,y -  (3030, 18)   (3030,)\n",
      "Dev X,y   -   (606, 18)    (606,)\n",
      "Test X,y  -   (404, 18)    (404,)\n",
      "Total:  4040\n"
     ]
    }
   ],
   "source": [
    "# Let's see the different tests\n",
    "\n",
    "print(\"Train X,y - \", X_train.shape, \" \", y_train.shape)\n",
    "print(\"Dev X,y   -  \", X_dev.shape, \"  \", y_dev.shape)\n",
    "print(\"Test X,y  -  \", X_test.shape, \"  \", y_test.shape)\n",
    "print(\"Total: \", X_train.shape[0] + X_dev.shape[0] +  X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162eaa71-855f-4e04-8c60-7e183adef961",
   "metadata": {},
   "source": [
    "# Pre-processing text with TFIDF vectorizer and SVD dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c77e8e-d161-462e-8e80-3c7cd8b0197e",
   "metadata": {},
   "source": [
    "Let's see first the text we are interested in analyzing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3081b42f-bafc-43e3-8dcc-ca99734909b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3956    Hello RAOP,\\n\\nfigured I'd give posting here a...\n",
       "2162    Hi RAOP!\\n\\nMy two roommates and I are all ful...\n",
       "2169    I've recently graduated university. I think I ...\n",
       "2027    So, I didn't find out my wife was leaving me t...\n",
       "483     (*throwaway acct*)\\nI never thought I'd get to...\n",
       "                              ...                        \n",
       "1899    First time on Random Acts of Pizza, my wallet ...\n",
       "3081    My son has recieved his meal, thanks veritas27...\n",
       "2723    Hey, I'm a broke college student, would really...\n",
       "2668    I live in Goodyear, AZ.\\n\\nI can receive pizza...\n",
       "1992    Well, my rent is paid up until June, and [my c...\n",
       "Name: request_text, Length: 3030, dtype: object"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check nan\n",
    "# pd.isnull(X_train['request_text']).sum()\n",
    "# X_train['request_text'].count()\n",
    "\n",
    "# See text messages\n",
    "X_train.loc[:,'request_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "773ebde5-2867-426b-a3da-968157049648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fills nan in text features\n",
    "# Input: array, array name\n",
    "# Output: fill array nan with 'the'\n",
    "def fill_nan_text(texts, feature):\n",
    "    texts = texts.replace(np.nan, 'the', regex=True)\n",
    "    return texts\n",
    "\n",
    "# Improve text tokenization\n",
    "# Input: string\n",
    "# Output: string without stop words, and applied\n",
    "# lemmatization and stemming\n",
    "def better_preprocessor(s):\n",
    "    \n",
    "    s = s.lower() # to lowercase\n",
    "    \n",
    "    #Stemming and lemmatization\n",
    "    ps = PorterStemmer()\n",
    "    lemmer = WordNetLemmatizer()\n",
    "    words = word_tokenize(s)\n",
    "    cleaned_words = [lemmer.lemmatize(ps.stem(word)) for word in words]\n",
    "    \n",
    "    #Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    new_cleaned_words = [w for w in cleaned_words if not w.lower() in stop_words]\n",
    "    s = ''\n",
    "    for word in new_cleaned_words:\n",
    "        if word == '.' or word == '?' or word == ',':\n",
    "            s = s + word\n",
    "        else:\n",
    "            s = s + ' ' + word\n",
    "    \n",
    "    return s\n",
    "\n",
    "# Vectorizes the text data into separated tokens as features\n",
    "# Input: array with text\n",
    "# Output: print size of vocabulary\n",
    "def create_text_features_no_processor(train):\n",
    "    \n",
    "    # Vectorization with empty preprocessing of text\n",
    "    vectorizer = CountVectorizer(preprocessor= lambda x: x)\n",
    "    X_train_counts_raw = vectorizer.fit_transform(train)\n",
    "    print('Size of train Vocabulary with empty pre-processor is:', X_train_counts_raw.shape)\n",
    "\n",
    "# Vectorizes the text data into separated tokens as features and applies better processor\n",
    "# Input: array with text\n",
    "# Output: print size of vocabulary\n",
    "def create_text_features_with_processor(train, test):\n",
    "    \n",
    "    # Vectorization with better preprocessor\n",
    "    # declare Vectorizer with counts of words\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "                                       analyzer='word',\n",
    "                                       token_pattern=r'\\w{1,}',\n",
    "#                                        min_df=3, \n",
    "                                       preprocessor = better_preprocessor,\n",
    "                                       ngram_range=(1,3))\n",
    "\n",
    "    #vectorizer on train data\n",
    "    X_train_counts = tfidf_vectorizer.fit_transform(fill_nan_text(train,'request_text'))\n",
    "    X_test = tfidf_vectorizer.transform(fill_nan_text(test,'request_text'))\n",
    "    print('---------- \\nSize of train Vocabulary with better preprocessor is:', X_train_counts.shape[1])\n",
    "    \n",
    "    #feature names\n",
    "    feature_names = tfidf_vectorizer.get_feature_names()\n",
    "#     print(len(feature_names))\n",
    "    return tfidf_vectorizer,X_train_counts, X_test, feature_names\n",
    "\n",
    "def svd_reduction(X_train_counts,X_test,svd_comps, feature_names):\n",
    "    \n",
    "    #SVD for dimensionality reduction\n",
    "    svd = TruncatedSVD(n_components=svd_comps,n_iter=10,random_state=42)\n",
    "    normalizer = Normalizer(copy=False) #Normalizing SVD results\n",
    "    lsa = make_pipeline(svd, normalizer) #making pipeline for svd and normalizer\n",
    "\n",
    "    ## reduced by SVD\n",
    "    X_svd = lsa.fit_transform(X_train_counts)\n",
    "    X_test_svd = lsa.transform(X_test)\n",
    "    print('---------- \\nSize of train Vocabulary with better preprocessor and SVD is:', X_svd.shape[1])\n",
    "    \n",
    "    #explained variance by SVD reduced dimensions\n",
    "    explained_variance = svd.explained_variance_ratio_.sum()\n",
    "    print(\"Explained variance of the SVD step: {}%\".format(int(explained_variance * 100)))\n",
    "    \n",
    "    #extract feature names corresponding to the 1500 SVD components\n",
    "    best_features = [feature_names[i] for i in svd.components_[0].argsort()[::-1]]\n",
    "    #print(best_features[:1500])\n",
    "    \n",
    "    print(f\"Train array: {X_svd.shape}\\nTest array: {X_test_svd.shape}\")\n",
    "    \n",
    "    #Convert to pandas df\n",
    "#     df_orig = pd.DataFrame(X_train_counts.toarray(), columns=feature_names)\n",
    "#     df = pd.DataFrame(df_orig, columns=best_features[:1500])\n",
    "  \n",
    "    return X_svd, X_test_svd\n",
    "\n",
    "def clean_array(array):\n",
    "    # Remove original text variables\n",
    "    array.pop('request_title')\n",
    "    array.pop('request_text')\n",
    "    # X_dev.columns\n",
    "    return array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc51459-a95f-479e-a764-8fb83641612c",
   "metadata": {},
   "source": [
    "# Merge meta and text data for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88efae4c-af03-4188-993c-7d14b467ad0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- \n",
      "Size of train Vocabulary with better preprocessor is: 196244\n",
      "---------- \n",
      "Size of train Vocabulary with better preprocessor and SVD is: 200\n",
      "Explained variance of the SVD step: 13%\n",
      "Train array: (3030, 200)\n",
      "Test array: (606, 200)\n",
      "Train meta: (3030, 16) - Train Text SVD: (3030, 200)\n",
      "Test meta: (606, 16) - Test Text SVD: (606, 200)\n",
      "Test meta + text: (3030, 216) - Test meta + text: (606, 216)\n",
      "**************************\n",
      "Logistic regression\n",
      "Accuracy for Logistic regression model is 0.85\n",
      "**************************\n",
      "Random forest classifier\n",
      "Accuracy (xg boost): 0.82 - d=7 - e=100\n",
      "Accuracy (xg boost): 0.82 - d=7 - e=150\n",
      "Accuracy (adaboost): 0.83 - d=7 - e=250\n",
      "Accuracy (xg boost): 0.81 - d=12 - e=100\n",
      "Accuracy (xg boost): 0.81 - d=12 - e=150\n",
      "Accuracy (xg boost): 0.82 - d=12 - e=250\n",
      "The best classifier is xg_ with acc=0.8201320132013201 - d=12 - e=250\n"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "\n",
    "svd_components = 200 # tested with 200, 400, 600, 1000, 1500\n",
    "cs=1 # tested with 0.01,0.1,1,10,100\n",
    "depths = [2,3,4]\n",
    "estimators = [15, 30, 80]\n",
    "best=[0,0,0,0]\n",
    "\n",
    "# Data Handling for Model\n",
    "\n",
    "X, Y = df.iloc[:,:-1], df.iloc[:,-1]\n",
    "\n",
    "# Shuffle the data, but make sure that the features and accompanying labels stay in sync.\n",
    "np.random.seed(0)\n",
    "shuffle = np.random.permutation(np.arange(X.shape[0]))\n",
    "X, Y = X.iloc[shuffle], Y.iloc[shuffle]\n",
    "\n",
    "# Split into train and test.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=1)\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(X_test, y_test, test_size=0.4, random_state=1)\n",
    "\n",
    "\n",
    "# Vectorize with Tfidf + SVD \n",
    "# Vectorize with TfIDF + SVD\n",
    "count_vector, train_counts, test_counts, features = create_text_features_with_processor(\n",
    "    X_train.loc[:,'request_text'], \n",
    "    X_dev.loc[:,'request_text'])\n",
    "\n",
    "train_svd, test_svd = svd_reduction(train_counts, test_counts,\n",
    "                                    svd_components,\n",
    "                                   features)\n",
    "\n",
    "# Clean text variables: title and request\n",
    "X_train = clean_array(X_train)\n",
    "X_dev = clean_array(X_dev)\n",
    "\n",
    "print(f\"Train meta: {X_train.shape} - Train Text SVD: {train_svd.shape}\") \n",
    "print(f\"Test meta: {X_dev.shape} - Test Text SVD: {test_svd.shape}\")\n",
    "\n",
    "X_train_mt = np.concatenate((X_train,train_svd), axis=1)\n",
    "X_dev_mt = np.concatenate((X_dev,test_svd), axis=1)\n",
    "print(f\"Test meta + text: {X_train_mt.shape} - Test meta + text: {X_dev_mt.shape}\")\n",
    "\n",
    "# fit model with L2 Regularization\n",
    "print(\"**************************\")\n",
    "print(\"Logistic regression\")\n",
    "lr_model = LogisticRegression(C=cs, max_iter= 1000, solver='liblinear',\n",
    "                             multi_class=\"auto\", penalty ='l2')\n",
    "lr_model.fit(X_train_mt, y_train)\n",
    "\n",
    "# predictions\n",
    "y_pred = lr_model.predict(X_dev_mt)\n",
    "\n",
    "# results\n",
    "#     cnf_matrix = metrics.confusion_matrix(y_dev, y_pred)\n",
    "# print(cnf_matrix)\n",
    "#     print(classification_report(y_dev, y_pred))\n",
    "report_log = classification_report(y_dev, y_pred, output_dict=True)\n",
    "print(\"Accuracy for Logistic regression model is\", round(report_log['accuracy'],2))\n",
    "print(\"**************************\")\n",
    "print(\"Random forest classifier\")\n",
    "\n",
    "for d in depths:\n",
    "    for e in estimators:\n",
    "        # XG Boost classifier\n",
    "        clf = xgb.XGBClassifier(max_depth=d, \n",
    "                                n_estimators=e, \n",
    "#                                 colsample_bytree=0.8,\n",
    "#                                 subsample=0.8, \n",
    "#                                 nthread=10, \n",
    "                                learning_rate=0.1,\n",
    "                                use_label_encoder=False,\n",
    "                                eval_metric='mlogloss')\n",
    "        clf.fit(X_train_mt, y_train)\n",
    "        predictions = clf.predict_proba(X_dev_mt)\n",
    "        if clf.score(X_dev_mt, y_dev) > best[0]:\n",
    "            print (f'Accuracy (xg boost): {clf.score(X_dev_mt, y_dev):.2} - d={d} - e={e}')\n",
    "            best[0] = clf.score(X_dev_mt, y_dev)\n",
    "            best[1] = d\n",
    "            best[2] = e\n",
    "            best[3] = 'xg_boost'\n",
    "        # Random Forest\n",
    "        rfc = RandomForestClassifier(n_estimators=e, max_depth=d)\n",
    "        rfc.fit(X_train_mt, y_train)\n",
    "        if clf.score(X_dev_mt, y_dev) > best[0]:\n",
    "            print (f'Accuracy (RF): {rfc.score(X_dev_mt, y_dev):.2} - d={d} - e={e}')\n",
    "            best[0] = rfc.score(X_dev_mt, y_dev)\n",
    "            best[1] = d\n",
    "            best[2] = e\n",
    "            best[3] = 'RF'\n",
    "            \n",
    "        # Ada Boost\n",
    "        abc = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=d)\n",
    "                                 , n_estimators=e, learning_rate=0.1)\n",
    "        abc.fit(X_train_mt, y_train)\n",
    "        if abc.score(X_dev_mt, y_dev) > best[0]:\n",
    "            print(f'Accuracy (adaboost): {abc.score(X_dev_mt, y_dev):.2} - d={d} - e={e}')\n",
    "            best[0] = rfc.score(X_dev_mt, y_dev)\n",
    "            best[1] = d\n",
    "            best[2] = e\n",
    "            best[3] = 'ada'\n",
    "\n",
    "print(\"**************************\")\n",
    "print(f\"The best classifier is {best[3]} with acc={best[0]:.3} - d={best[1]} - e={best[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f5a91d-0806-4971-9bfd-4afaafae8e6e",
   "metadata": {},
   "source": [
    "# Old code. We can remove in the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "66d938b0-8c8c-4807-ac6f-0f0126c25b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test meta + text: (3030, 217) - Test meta + text: (606, 217)\n"
     ]
    }
   ],
   "source": [
    "# # Remove original text variables\n",
    "# X_train.pop('request_title')\n",
    "# X_train.pop('request_text')\n",
    "# X_dev.pop('request_title')\n",
    "# X_dev.pop('request_text')\n",
    "# # X_dev.columns\n",
    "\n",
    "# X_train_mt = np.concatenate((X_train,train_svd), axis=1)\n",
    "# X_dev_mt = np.concatenate((X_dev,test_svd), axis=1)\n",
    "\n",
    "\n",
    "# print(f\"Test meta + text: {X_train_mt.shape} - Test meta + text: {X_dev_mt.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8ebeec78-6e09-4776-b2d1-7aa3a13b53c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_mt[1,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8f1ec0d1-bdcc-4bf0-9f4b-ef5e8c4bde4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.87      0.94      0.90       455\n",
      "        True       0.77      0.57      0.65       151\n",
      "\n",
      "    accuracy                           0.85       606\n",
      "   macro avg       0.82      0.76      0.78       606\n",
      "weighted avg       0.84      0.85      0.84       606\n",
      "\n",
      "Accuracy for Logistic regression model is 0.85\n"
     ]
    }
   ],
   "source": [
    "# # fit model\n",
    "# lr_model = LogisticRegression(max_iter= 1000, solver='liblinear')\n",
    "# lr_model.fit(X_train_mt[:,1:], y_train)\n",
    "\n",
    "# # predictions\n",
    "# y_pred = lr_model.predict(X_dev_mt[:,1:])\n",
    "\n",
    "# # results\n",
    "# cnf_matrix = metrics.confusion_matrix(y_dev, y_pred)\n",
    "# # print(cnf_matrix)\n",
    "# print(classification_report(y_dev, y_pred))\n",
    "# report_log = classification_report(y_dev, y_pred, output_dict=True)\n",
    "# print(\"Accuracy for Logistic regression model is\", round(report_log['accuracy'],2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "21325d5d-ba24-4552-a2b6-63be2a9091a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (a random forest): 0.7706270627062707\n",
      "Accuracy (adaboost with decision trees): 0.8283828382838284\n"
     ]
    }
   ],
   "source": [
    "# # dt = DecisionTreeClassifier(criterion=\"entropy\", splitter=\"best\", random_state=0)\n",
    "# # dt.fit(X_train_mt[:,1:], y_train)\n",
    "\n",
    "# # print ('Accuracy (a decision tree):', dt.score(X_dev_mt[:,1:], y_dev))\n",
    "\n",
    "# rfc = RandomForestClassifier(n_estimators=20, max_depth=4)\n",
    "# rfc.fit(X_train_mt[:,1:], y_train)\n",
    "\n",
    "# print ('Accuracy (a random forest):', rfc.score(X_dev_mt[:,1:], y_dev))\n",
    "\n",
    "# abc = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), n_estimators=50, learning_rate=0.1)\n",
    "\n",
    "# abc.fit(X_train_mt[:,1:], y_train)\n",
    "# print ('Accuracy (adaboost with decision trees):', abc.score(X_dev_mt[:,1:], y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15f8e58-f439-4328-a8c1-cec9d82bd61e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
