{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# Import a bunch of libraries.\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.23.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>giver_username_if_known</th>\n",
       "      <th>number_of_downvotes_of_request_at_retrieval</th>\n",
       "      <th>number_of_upvotes_of_request_at_retrieval</th>\n",
       "      <th>post_was_edited</th>\n",
       "      <th>request_id</th>\n",
       "      <th>request_number_of_comments_at_retrieval</th>\n",
       "      <th>request_text</th>\n",
       "      <th>request_text_edit_aware</th>\n",
       "      <th>request_title</th>\n",
       "      <th>requester_account_age_in_days_at_request</th>\n",
       "      <th>...</th>\n",
       "      <th>requester_received_pizza</th>\n",
       "      <th>requester_subreddits_at_request</th>\n",
       "      <th>requester_upvotes_minus_downvotes_at_request</th>\n",
       "      <th>requester_upvotes_minus_downvotes_at_retrieval</th>\n",
       "      <th>requester_upvotes_plus_downvotes_at_request</th>\n",
       "      <th>requester_upvotes_plus_downvotes_at_retrieval</th>\n",
       "      <th>requester_user_flair</th>\n",
       "      <th>requester_username</th>\n",
       "      <th>unix_timestamp_of_request</th>\n",
       "      <th>unix_timestamp_of_request_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>t3_l25d7</td>\n",
       "      <td>0</td>\n",
       "      <td>Hi I am in need of food for my 4 children we a...</td>\n",
       "      <td>Hi I am in need of food for my 4 children we a...</td>\n",
       "      <td>Request Colorado Springs Help Us Please</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>nickylvst</td>\n",
       "      <td>1317852607</td>\n",
       "      <td>1317849007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N/A</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>t3_rcb83</td>\n",
       "      <td>0</td>\n",
       "      <td>I spent the last money I had on gas today. Im ...</td>\n",
       "      <td>I spent the last money I had on gas today. Im ...</td>\n",
       "      <td>[Request] California, No cash and I could use ...</td>\n",
       "      <td>501.111100</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>[AskReddit, Eve, IAmA, MontereyBay, RandomKind...</td>\n",
       "      <td>34</td>\n",
       "      <td>4258</td>\n",
       "      <td>116</td>\n",
       "      <td>11168</td>\n",
       "      <td>None</td>\n",
       "      <td>fohacidal</td>\n",
       "      <td>1332652424</td>\n",
       "      <td>1332648824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>t3_lpu5j</td>\n",
       "      <td>0</td>\n",
       "      <td>My girlfriend decided it would be a good idea ...</td>\n",
       "      <td>My girlfriend decided it would be a good idea ...</td>\n",
       "      <td>[Request] Hungry couple in Dundee, Scotland wo...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>jacquibatman7</td>\n",
       "      <td>1319650094</td>\n",
       "      <td>1319646494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>t3_mxvj3</td>\n",
       "      <td>4</td>\n",
       "      <td>It's cold, I'n hungry, and to be completely ho...</td>\n",
       "      <td>It's cold, I'n hungry, and to be completely ho...</td>\n",
       "      <td>[Request] In Canada (Ontario), just got home f...</td>\n",
       "      <td>6.518438</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>[AskReddit, DJs, IAmA, Random_Acts_Of_Pizza]</td>\n",
       "      <td>54</td>\n",
       "      <td>59</td>\n",
       "      <td>76</td>\n",
       "      <td>81</td>\n",
       "      <td>None</td>\n",
       "      <td>4on_the_floor</td>\n",
       "      <td>1322855434</td>\n",
       "      <td>1322855434</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  giver_username_if_known  number_of_downvotes_of_request_at_retrieval  \\\n",
       "0                     N/A                                            0   \n",
       "1                     N/A                                            2   \n",
       "2                     N/A                                            0   \n",
       "3                     N/A                                            0   \n",
       "\n",
       "   number_of_upvotes_of_request_at_retrieval  post_was_edited request_id  \\\n",
       "0                                          1                0   t3_l25d7   \n",
       "1                                          5                0   t3_rcb83   \n",
       "2                                          3                0   t3_lpu5j   \n",
       "3                                          1                1   t3_mxvj3   \n",
       "\n",
       "   request_number_of_comments_at_retrieval  \\\n",
       "0                                        0   \n",
       "1                                        0   \n",
       "2                                        0   \n",
       "3                                        4   \n",
       "\n",
       "                                        request_text  \\\n",
       "0  Hi I am in need of food for my 4 children we a...   \n",
       "1  I spent the last money I had on gas today. Im ...   \n",
       "2  My girlfriend decided it would be a good idea ...   \n",
       "3  It's cold, I'n hungry, and to be completely ho...   \n",
       "\n",
       "                             request_text_edit_aware  \\\n",
       "0  Hi I am in need of food for my 4 children we a...   \n",
       "1  I spent the last money I had on gas today. Im ...   \n",
       "2  My girlfriend decided it would be a good idea ...   \n",
       "3  It's cold, I'n hungry, and to be completely ho...   \n",
       "\n",
       "                                       request_title  \\\n",
       "0            Request Colorado Springs Help Us Please   \n",
       "1  [Request] California, No cash and I could use ...   \n",
       "2  [Request] Hungry couple in Dundee, Scotland wo...   \n",
       "3  [Request] In Canada (Ontario), just got home f...   \n",
       "\n",
       "   requester_account_age_in_days_at_request  ...  requester_received_pizza  \\\n",
       "0                                  0.000000  ...                     False   \n",
       "1                                501.111100  ...                     False   \n",
       "2                                  0.000000  ...                     False   \n",
       "3                                  6.518438  ...                     False   \n",
       "\n",
       "                     requester_subreddits_at_request  \\\n",
       "0                                                 []   \n",
       "1  [AskReddit, Eve, IAmA, MontereyBay, RandomKind...   \n",
       "2                                                 []   \n",
       "3       [AskReddit, DJs, IAmA, Random_Acts_Of_Pizza]   \n",
       "\n",
       "   requester_upvotes_minus_downvotes_at_request  \\\n",
       "0                                             0   \n",
       "1                                            34   \n",
       "2                                             0   \n",
       "3                                            54   \n",
       "\n",
       "   requester_upvotes_minus_downvotes_at_retrieval  \\\n",
       "0                                               1   \n",
       "1                                            4258   \n",
       "2                                               3   \n",
       "3                                              59   \n",
       "\n",
       "   requester_upvotes_plus_downvotes_at_request  \\\n",
       "0                                            0   \n",
       "1                                          116   \n",
       "2                                            0   \n",
       "3                                           76   \n",
       "\n",
       "   requester_upvotes_plus_downvotes_at_retrieval  requester_user_flair  \\\n",
       "0                                              1                  None   \n",
       "1                                          11168                  None   \n",
       "2                                              3                  None   \n",
       "3                                             81                  None   \n",
       "\n",
       "   requester_username  unix_timestamp_of_request  \\\n",
       "0           nickylvst                 1317852607   \n",
       "1           fohacidal                 1332652424   \n",
       "2       jacquibatman7                 1319650094   \n",
       "3       4on_the_floor                 1322855434   \n",
       "\n",
       "   unix_timestamp_of_request_utc  \n",
       "0                     1317849007  \n",
       "1                     1332648824  \n",
       "2                     1319646494  \n",
       "3                     1322855434  \n",
       "\n",
       "[4 rows x 32 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_raw = pd.read_json('/Users/kanikamahajan/Desktop/MIDS/W207/Projects/Final_project_data/train.json')\n",
    "test_data_raw = pd.read_json('/Users/kanikamahajan/Desktop/MIDS/W207/Projects/Final_project_data/test.json')\n",
    "train_data_raw.head(4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating day of request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate result using pandas\n",
    "day_request = []\n",
    "for day in train_data_raw[\"unix_timestamp_of_request\"]:\n",
    "    date = time.ctime(int(day))\n",
    "    day = date[0:3]\n",
    "    day_request.append(day)\n",
    "\n",
    "train_data_raw[\"day_of_request\"] = day_request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final text processing\n",
    "* Text pre-processing (removing punctuations, lower case etc.)\n",
    "* NLTK stemming, lemetization\n",
    "* TfidfVectorizer\n",
    "* Latent Semantic Analysis and SVD for dimensionality reduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# x = train_data_raw.iloc[:, 6] #features\n",
    "# x = StandardScaler().fit_transform(x) # normalizing the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data into train, dev and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3030,)\n"
     ]
    }
   ],
   "source": [
    "# Only lookign at text vector, using text as features and testing model accuracy\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Extracting text from df (x) and outcome y\n",
    "\n",
    "x = train_data_raw.iloc[:, 6] #features\n",
    "y = train_data_raw.iloc[:, 22] #outcome\n",
    "\n",
    "#test data does not have the outcome column so splitting training data into test, dev and training\n",
    "train_ratio = 0.75\n",
    "validation_ratio = 0.15\n",
    "test_ratio = 0.10\n",
    "\n",
    "# train is now 75% of the entire data set\n",
    "# the _junk suffix means that we drop that variable completely\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(x, y, test_size=1 - train_ratio, random_state=1)\n",
    "\n",
    "# test is now 10% of the initial data set\n",
    "# validation is now 15% of the initial data set\n",
    "dev_data, test_data, dev_labels, test_labels = train_test_split(test_data, test_labels, test_size=test_ratio/(test_ratio + validation_ratio), random_state=1) \n",
    "\n",
    "#print(X_train, X_dev, X_test)\n",
    "print(train_data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing text with TFIDF vectorizer and SVD dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train Vocabulary with empty pre-processor is: (3030, 12637)\n",
      "---------- \n",
      " Size of train Vocabulary with better preprocessor is: (3030, 8133)\n",
      "8133\n",
      "---------- \n",
      " Size of train Vocabulary with better preprocessor and SVD is: (3030, 1500)\n",
      "Explained variance of the SVD step: 90%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pizza</th>\n",
       "      <th>would</th>\n",
       "      <th>thi</th>\n",
       "      <th>get</th>\n",
       "      <th>help</th>\n",
       "      <th>thank</th>\n",
       "      <th>realli</th>\n",
       "      <th>week</th>\n",
       "      <th>pay</th>\n",
       "      <th>food</th>\n",
       "      <th>...</th>\n",
       "      <th>mouth</th>\n",
       "      <th>toronto</th>\n",
       "      <th>honor</th>\n",
       "      <th>attack</th>\n",
       "      <th>variou</th>\n",
       "      <th>event</th>\n",
       "      <th>scholarship</th>\n",
       "      <th>film</th>\n",
       "      <th>36</th>\n",
       "      <th>americorp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.295420</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.24076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.037988</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054353</td>\n",
       "      <td>0.062028</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.135567</td>\n",
       "      <td>0.118212</td>\n",
       "      <td>0.045682</td>\n",
       "      <td>0.046379</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.048492</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.054036</td>\n",
       "      <td>0.188473</td>\n",
       "      <td>0.072834</td>\n",
       "      <td>0.073945</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.088232</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.112187</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.075608</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080258</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3025</th>\n",
       "      <td>0.037165</td>\n",
       "      <td>0.086419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.109709</td>\n",
       "      <td>0.106350</td>\n",
       "      <td>0.060684</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3026</th>\n",
       "      <td>0.225919</td>\n",
       "      <td>0.065666</td>\n",
       "      <td>0.076128</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080811</td>\n",
       "      <td>0.184447</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3027</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3028</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064975</td>\n",
       "      <td>0.197900</td>\n",
       "      <td>0.142301</td>\n",
       "      <td>0.137943</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160915</td>\n",
       "      <td>0.244596</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3029</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064889</td>\n",
       "      <td>0.075228</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.093153</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3030 rows Ã— 1500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         pizza     would       thi       get      help     thank    realli  \\\n",
       "0     0.000000  0.000000  0.000000  0.295420  0.000000  0.000000  0.000000   \n",
       "1     0.037988  0.000000  0.000000  0.000000  0.000000  0.054353  0.062028   \n",
       "2     0.135567  0.118212  0.045682  0.046379  0.000000  0.048492  0.000000   \n",
       "3     0.054036  0.188473  0.072834  0.073945  0.000000  0.000000  0.088232   \n",
       "4     0.112187  0.065217  0.075608  0.000000  0.000000  0.080258  0.000000   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "3025  0.037165  0.086419  0.000000  0.000000  0.109709  0.106350  0.060684   \n",
       "3026  0.225919  0.065666  0.076128  0.000000  0.000000  0.080811  0.184447   \n",
       "3027  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3028  0.000000  0.000000  0.064975  0.197900  0.142301  0.137943  0.000000   \n",
       "3029  0.000000  0.064889  0.075228  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "          week       pay     food  ...  mouth  toronto  honor  attack  variou  \\\n",
       "0     0.000000  0.000000  0.24076  ...    0.0      0.0    0.0     0.0     0.0   \n",
       "1     0.000000  0.000000  0.00000  ...    0.0      0.0    0.0     0.0     0.0   \n",
       "2     0.000000  0.000000  0.00000  ...    0.0      0.0    0.0     0.0     0.0   \n",
       "3     0.000000  0.000000  0.00000  ...    0.0      0.0    0.0     0.0     0.0   \n",
       "4     0.000000  0.000000  0.00000  ...    0.0      0.0    0.0     0.0     0.0   \n",
       "...        ...       ...      ...  ...    ...      ...    ...     ...     ...   \n",
       "3025  0.000000  0.000000  0.00000  ...    0.0      0.0    0.0     0.0     0.0   \n",
       "3026  0.000000  0.000000  0.00000  ...    0.0      0.0    0.0     0.0     0.0   \n",
       "3027  0.000000  0.000000  0.00000  ...    0.0      0.0    0.0     0.0     0.0   \n",
       "3028  0.160915  0.244596  0.00000  ...    0.0      0.0    0.0     0.0     0.0   \n",
       "3029  0.093153  0.000000  0.00000  ...    0.0      0.0    0.0     0.0     0.0   \n",
       "\n",
       "      event  scholarship  film   36  americorp  \n",
       "0       0.0          0.0   0.0  0.0        0.0  \n",
       "1       0.0          0.0   0.0  0.0        0.0  \n",
       "2       0.0          0.0   0.0  0.0        0.0  \n",
       "3       0.0          0.0   0.0  0.0        0.0  \n",
       "4       0.0          0.0   0.0  0.0        0.0  \n",
       "...     ...          ...   ...  ...        ...  \n",
       "3025    0.0          0.0   0.0  0.0        0.0  \n",
       "3026    0.0          0.0   0.0  0.0        0.0  \n",
       "3027    0.0          0.0   0.0  0.0        0.0  \n",
       "3028    0.0          0.0   0.0  0.0        0.0  \n",
       "3029    0.0          0.0   0.0  0.0        0.0  \n",
       "\n",
       "[3030 rows x 1500 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def better_preprocessor(s):\n",
    "    \n",
    "    ### STUDENT START ###\n",
    "    s = s.lower() # to lowercase\n",
    "    \n",
    "    #Stemming and lemmatization\n",
    "    ps = PorterStemmer()\n",
    "    lemmer = WordNetLemmatizer()\n",
    "    words = word_tokenize(s)\n",
    "    cleaned_words = [lemmer.lemmatize(ps.stem(word)) for word in words]\n",
    "    \n",
    "    #Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    new_cleaned_words = [w for w in cleaned_words if not w.lower() in stop_words]\n",
    "    s = ''\n",
    "    for word in new_cleaned_words:\n",
    "        if word == '.' or word == '?' or word == ',':\n",
    "            s = s + word\n",
    "        else:\n",
    "            s = s + ' ' + word\n",
    "    \n",
    "    return s\n",
    "    ### STUDENT END ###\n",
    "\n",
    "\n",
    "def P5():\n",
    "    ### STUDENT START ###\n",
    "    \n",
    "    # Vectorization with empty preprocessing of text\n",
    "    vectorizer = CountVectorizer(preprocessor= lambda x: x)\n",
    "    X_train_counts_raw = vectorizer.fit_transform(train_data )\n",
    "    print('Size of train Vocabulary with empty pre-processor is:', X_train_counts_raw.shape)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Vectorization with better preprocessor\n",
    "    \n",
    "    #declare Vectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer(preprocessor = better_preprocessor)\n",
    "\n",
    "    #vectorizer on train data\n",
    "    X_train_counts = tfidf_vectorizer.fit_transform(train_data)\n",
    "    print('---------- \\n Size of train Vocabulary with better preprocessor is:', X_train_counts.shape)\n",
    "    \n",
    "    #feature names\n",
    "    feature_names = tfidf_vectorizer.get_feature_names()\n",
    "    print(len(feature_names))\n",
    "    \n",
    "    #SVD for dimensionality reduction\n",
    "    svd = TruncatedSVD(n_components=1500,n_iter=10,random_state=42)\n",
    "    normalizer = Normalizer(copy=False) #Normalizing SVD results\n",
    "    lsa = make_pipeline(svd, normalizer) #making pipeline for svd and normalizer\n",
    "    \n",
    "    ## reduced by SVD\n",
    "    X_svd = lsa.fit_transform(X_train_counts)\n",
    "    print('---------- \\n Size of train Vocabulary with better preprocessor and SVD is:', X_svd.shape)\n",
    "    \n",
    "    #explained variance by SVD reduced dimensions\n",
    "    explained_variance = svd.explained_variance_ratio_.sum()\n",
    "    print(\"Explained variance of the SVD step: {}%\".format(int(explained_variance * 100)))\n",
    "    \n",
    "    #extract feature names corresponding to the 1500 SVD components\n",
    "    best_features = [feature_names[i] for i in svd.components_[0].argsort()[::-1]]\n",
    "    #print(best_features[:1500])\n",
    "    \n",
    "    #print(type(X_train_counts.toarray()))\n",
    "    \n",
    "    #Convert to pandas df\n",
    "    df_orig = pd.DataFrame(X_train_counts.toarray(), columns=feature_names)\n",
    "    df = pd.DataFrame(df_orig, columns=best_features[:1500])\n",
    "  \n",
    "    return (df)\n",
    "    ### STUDENT END ###\n",
    "\n",
    "P5()\n",
    "# Gridsearch : https://medium.com/swlh/a-text-classification-approach-using-vector-space-modelling-doc2vec-pca-74fb6fd73760\n",
    "# Link Main: https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "#SVD: https://scikit-learn.org/stable/modules/decomposition.html\n",
    "#Explained variance: https://scikit-learn.org/stable/auto_examples/text/plot_document_clustering.html#sphx-glr-auto-examples-text-plot-document-clustering-py\n",
    "#writeout featurenames: https://stackoverflow.com/questions/44633571/how-can-i-get-the-feature-names-from-sklearn-truncatedsvd-object/49055864"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "Text_features = P5()\n",
    "Text_features.to_csv('/Users/kanikamahajan/Desktop/MIDS/W207/Projects/text_processed_features.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Combine new text processed features with main dataframe\n",
    "\n",
    "df_text_time_merge = pd.concat([train_data_raw, Text_features], axis=1, join=\"inner\")\n",
    "df_text_time_merge.to_csv('/Users/kanikamahajan/Desktop/MIDS/W207/Projects/df_text_time_merge.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTIPLE CLASSIFIERS TO SEE ACCURACY WITH ONLY TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer => transformer => classifier pipeline using pipeline class in sklearn\n",
    "#Logistic Regression\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', LogisticRegression(penalty='l2', C=0.5,solver=\"liblinear\", multi_class=\"auto\")),])\n",
    "text_clf.fit(train_data, train_labels)\n",
    "predicted = text_clf.predict(dev_data)\n",
    "np.mean(predicted == dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42,max_iter=5, tol=None)),])\n",
    "text_clf.fit(train_data, train_labels)\n",
    "predicted = text_clf.predict(dev_data)\n",
    "np.mean(predicted == dev_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRID Search for tuning hyperparameters\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "                  'tfidf__use_idf': (True, False),\n",
    "                  'clf__alpha': (1e-2, 1e-3),\n",
    "                 }\n",
    "\n",
    "gs_clf = GridSearchCV(text_clf, parameters, cv=5, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(train_data, train_labels)\n",
    "train_data[gs_clf.predict()]\n",
    "gs_clf.best_score_\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NB\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', BernoulliNB()),])\n",
    "text_clf.fit(train_data, train_labels)\n",
    "predicted = text_clf.predict(dev_data)\n",
    "np.mean(predicted == dev_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K nearest neighbors\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', KNeighborsClassifier(n_neighbors=5)),])\n",
    "text_clf.fit(train_data, train_labels)\n",
    "predicted = text_clf.predict(dev_data)\n",
    "np.mean(predicted == dev_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi layer Neural networks\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(5, 2), random_state=1)),])\n",
    "text_clf.fit(train_data, train_labels)\n",
    "predicted = text_clf.predict(dev_data)\n",
    "np.mean(predicted == dev_labels)\n",
    "\n",
    "#Link: https://scikit-learn.org/stable/modules/neural_networks_supervised.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
