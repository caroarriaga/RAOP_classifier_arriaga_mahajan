{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# Import a bunch of libraries.\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.23.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>giver_username_if_known</th>\n",
       "      <th>number_of_downvotes_of_request_at_retrieval</th>\n",
       "      <th>number_of_upvotes_of_request_at_retrieval</th>\n",
       "      <th>post_was_edited</th>\n",
       "      <th>request_id</th>\n",
       "      <th>request_number_of_comments_at_retrieval</th>\n",
       "      <th>request_text</th>\n",
       "      <th>request_text_edit_aware</th>\n",
       "      <th>request_title</th>\n",
       "      <th>requester_account_age_in_days_at_request</th>\n",
       "      <th>...</th>\n",
       "      <th>requester_received_pizza</th>\n",
       "      <th>requester_subreddits_at_request</th>\n",
       "      <th>requester_upvotes_minus_downvotes_at_request</th>\n",
       "      <th>requester_upvotes_minus_downvotes_at_retrieval</th>\n",
       "      <th>requester_upvotes_plus_downvotes_at_request</th>\n",
       "      <th>requester_upvotes_plus_downvotes_at_retrieval</th>\n",
       "      <th>requester_user_flair</th>\n",
       "      <th>requester_username</th>\n",
       "      <th>unix_timestamp_of_request</th>\n",
       "      <th>unix_timestamp_of_request_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>t3_l25d7</td>\n",
       "      <td>0</td>\n",
       "      <td>Hi I am in need of food for my 4 children we a...</td>\n",
       "      <td>Hi I am in need of food for my 4 children we a...</td>\n",
       "      <td>Request Colorado Springs Help Us Please</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>nickylvst</td>\n",
       "      <td>1317852607</td>\n",
       "      <td>1317849007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N/A</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>t3_rcb83</td>\n",
       "      <td>0</td>\n",
       "      <td>I spent the last money I had on gas today. Im ...</td>\n",
       "      <td>I spent the last money I had on gas today. Im ...</td>\n",
       "      <td>[Request] California, No cash and I could use ...</td>\n",
       "      <td>501.111100</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>[AskReddit, Eve, IAmA, MontereyBay, RandomKind...</td>\n",
       "      <td>34</td>\n",
       "      <td>4258</td>\n",
       "      <td>116</td>\n",
       "      <td>11168</td>\n",
       "      <td>None</td>\n",
       "      <td>fohacidal</td>\n",
       "      <td>1332652424</td>\n",
       "      <td>1332648824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>t3_lpu5j</td>\n",
       "      <td>0</td>\n",
       "      <td>My girlfriend decided it would be a good idea ...</td>\n",
       "      <td>My girlfriend decided it would be a good idea ...</td>\n",
       "      <td>[Request] Hungry couple in Dundee, Scotland wo...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>jacquibatman7</td>\n",
       "      <td>1319650094</td>\n",
       "      <td>1319646494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>t3_mxvj3</td>\n",
       "      <td>4</td>\n",
       "      <td>It's cold, I'n hungry, and to be completely ho...</td>\n",
       "      <td>It's cold, I'n hungry, and to be completely ho...</td>\n",
       "      <td>[Request] In Canada (Ontario), just got home f...</td>\n",
       "      <td>6.518438</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>[AskReddit, DJs, IAmA, Random_Acts_Of_Pizza]</td>\n",
       "      <td>54</td>\n",
       "      <td>59</td>\n",
       "      <td>76</td>\n",
       "      <td>81</td>\n",
       "      <td>None</td>\n",
       "      <td>4on_the_floor</td>\n",
       "      <td>1322855434</td>\n",
       "      <td>1322855434</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  giver_username_if_known  number_of_downvotes_of_request_at_retrieval  \\\n",
       "0                     N/A                                            0   \n",
       "1                     N/A                                            2   \n",
       "2                     N/A                                            0   \n",
       "3                     N/A                                            0   \n",
       "\n",
       "   number_of_upvotes_of_request_at_retrieval  post_was_edited request_id  \\\n",
       "0                                          1                0   t3_l25d7   \n",
       "1                                          5                0   t3_rcb83   \n",
       "2                                          3                0   t3_lpu5j   \n",
       "3                                          1                1   t3_mxvj3   \n",
       "\n",
       "   request_number_of_comments_at_retrieval  \\\n",
       "0                                        0   \n",
       "1                                        0   \n",
       "2                                        0   \n",
       "3                                        4   \n",
       "\n",
       "                                        request_text  \\\n",
       "0  Hi I am in need of food for my 4 children we a...   \n",
       "1  I spent the last money I had on gas today. Im ...   \n",
       "2  My girlfriend decided it would be a good idea ...   \n",
       "3  It's cold, I'n hungry, and to be completely ho...   \n",
       "\n",
       "                             request_text_edit_aware  \\\n",
       "0  Hi I am in need of food for my 4 children we a...   \n",
       "1  I spent the last money I had on gas today. Im ...   \n",
       "2  My girlfriend decided it would be a good idea ...   \n",
       "3  It's cold, I'n hungry, and to be completely ho...   \n",
       "\n",
       "                                       request_title  \\\n",
       "0            Request Colorado Springs Help Us Please   \n",
       "1  [Request] California, No cash and I could use ...   \n",
       "2  [Request] Hungry couple in Dundee, Scotland wo...   \n",
       "3  [Request] In Canada (Ontario), just got home f...   \n",
       "\n",
       "   requester_account_age_in_days_at_request  ...  requester_received_pizza  \\\n",
       "0                                  0.000000  ...                     False   \n",
       "1                                501.111100  ...                     False   \n",
       "2                                  0.000000  ...                     False   \n",
       "3                                  6.518438  ...                     False   \n",
       "\n",
       "                     requester_subreddits_at_request  \\\n",
       "0                                                 []   \n",
       "1  [AskReddit, Eve, IAmA, MontereyBay, RandomKind...   \n",
       "2                                                 []   \n",
       "3       [AskReddit, DJs, IAmA, Random_Acts_Of_Pizza]   \n",
       "\n",
       "   requester_upvotes_minus_downvotes_at_request  \\\n",
       "0                                             0   \n",
       "1                                            34   \n",
       "2                                             0   \n",
       "3                                            54   \n",
       "\n",
       "   requester_upvotes_minus_downvotes_at_retrieval  \\\n",
       "0                                               1   \n",
       "1                                            4258   \n",
       "2                                               3   \n",
       "3                                              59   \n",
       "\n",
       "   requester_upvotes_plus_downvotes_at_request  \\\n",
       "0                                            0   \n",
       "1                                          116   \n",
       "2                                            0   \n",
       "3                                           76   \n",
       "\n",
       "   requester_upvotes_plus_downvotes_at_retrieval  requester_user_flair  \\\n",
       "0                                              1                  None   \n",
       "1                                          11168                  None   \n",
       "2                                              3                  None   \n",
       "3                                             81                  None   \n",
       "\n",
       "   requester_username  unix_timestamp_of_request  \\\n",
       "0           nickylvst                 1317852607   \n",
       "1           fohacidal                 1332652424   \n",
       "2       jacquibatman7                 1319650094   \n",
       "3       4on_the_floor                 1322855434   \n",
       "\n",
       "   unix_timestamp_of_request_utc  \n",
       "0                     1317849007  \n",
       "1                     1332648824  \n",
       "2                     1319646494  \n",
       "3                     1322855434  \n",
       "\n",
       "[4 rows x 32 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_raw = pd.read_json('/Users/kanikamahajan/Desktop/MIDS/W207/Projects/Final_project_data/train.json')\n",
    "test_data_raw = pd.read_json('/Users/kanikamahajan/Desktop/MIDS/W207/Projects/Final_project_data/test.json')\n",
    "train_data_raw.head(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate result using pandas\n",
    "day_request = []\n",
    "for day in train_data_raw[\"unix_timestamp_of_request\"]:\n",
    "    date = time.ctime(int(day))\n",
    "    day = date[0:3]\n",
    "    day_request.append(day)\n",
    "\n",
    "train_data_raw[\"day_of_request\"] = day_request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final text processing\n",
    "* Text pre-processing (removing punctuations, lower case etc.)\n",
    "* NLTK stemming, lemetization\n",
    "* TfidfVectorizer\n",
    "* Latent Semantic Analysis and SVD for dimensionality reduction\n",
    "* unix_timestamp_of_request_utc to day of week\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "# x = train_data_raw.iloc[:, 6] #features\n",
    "# x = StandardScaler().fit_transform(x) # normalizing the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3030,)\n"
     ]
    }
   ],
   "source": [
    "# Only lookign at text vector, using text as features and testing model accuracy\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Extracting text from df (x) and outcome y\n",
    "\n",
    "x = train_data_raw.iloc[:, 6] #features\n",
    "y = train_data_raw.iloc[:, 22] #outcome\n",
    "\n",
    "#test data does not have the outcome column so splitting training data into test, dev and training\n",
    "train_ratio = 0.75\n",
    "validation_ratio = 0.15\n",
    "test_ratio = 0.10\n",
    "\n",
    "# train is now 75% of the entire data set\n",
    "# the _junk suffix means that we drop that variable completely\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(x, y, test_size=1 - train_ratio, random_state=1)\n",
    "\n",
    "# test is now 10% of the initial data set\n",
    "# validation is now 15% of the initial data set\n",
    "dev_data, test_data, dev_labels, test_labels = train_test_split(test_data, test_labels, test_size=test_ratio/(test_ratio + validation_ratio), random_state=1) \n",
    "\n",
    "#print(X_train, X_dev, X_test)\n",
    "print(train_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train Vocabulary with empty pre-processor is: (3030, 12637)\n",
      "---------- \n",
      " Size of train Vocabulary with better preprocessor is: (3030, 8133)\n",
      "8133\n",
      "---------- \n",
      " Size of train Vocabulary with better preprocessor and SVD is: (3030, 1500)\n",
      "Explained variance of the SVD step: 90%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pizza</th>\n",
       "      <th>would</th>\n",
       "      <th>thi</th>\n",
       "      <th>get</th>\n",
       "      <th>help</th>\n",
       "      <th>thank</th>\n",
       "      <th>realli</th>\n",
       "      <th>week</th>\n",
       "      <th>pay</th>\n",
       "      <th>food</th>\n",
       "      <th>...</th>\n",
       "      <th>mouth</th>\n",
       "      <th>toronto</th>\n",
       "      <th>honor</th>\n",
       "      <th>attack</th>\n",
       "      <th>variou</th>\n",
       "      <th>event</th>\n",
       "      <th>scholarship</th>\n",
       "      <th>film</th>\n",
       "      <th>36</th>\n",
       "      <th>americorp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.295420</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.24076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.037988</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054353</td>\n",
       "      <td>0.062028</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.135567</td>\n",
       "      <td>0.118212</td>\n",
       "      <td>0.045682</td>\n",
       "      <td>0.046379</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.048492</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.054036</td>\n",
       "      <td>0.188473</td>\n",
       "      <td>0.072834</td>\n",
       "      <td>0.073945</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.088232</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.112187</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.075608</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080258</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3025</th>\n",
       "      <td>0.037165</td>\n",
       "      <td>0.086419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.109709</td>\n",
       "      <td>0.106350</td>\n",
       "      <td>0.060684</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3026</th>\n",
       "      <td>0.225919</td>\n",
       "      <td>0.065666</td>\n",
       "      <td>0.076128</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080811</td>\n",
       "      <td>0.184447</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3027</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3028</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064975</td>\n",
       "      <td>0.197900</td>\n",
       "      <td>0.142301</td>\n",
       "      <td>0.137943</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160915</td>\n",
       "      <td>0.244596</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3029</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064889</td>\n",
       "      <td>0.075228</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.093153</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3030 rows × 1500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         pizza     would       thi       get      help     thank    realli  \\\n",
       "0     0.000000  0.000000  0.000000  0.295420  0.000000  0.000000  0.000000   \n",
       "1     0.037988  0.000000  0.000000  0.000000  0.000000  0.054353  0.062028   \n",
       "2     0.135567  0.118212  0.045682  0.046379  0.000000  0.048492  0.000000   \n",
       "3     0.054036  0.188473  0.072834  0.073945  0.000000  0.000000  0.088232   \n",
       "4     0.112187  0.065217  0.075608  0.000000  0.000000  0.080258  0.000000   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "3025  0.037165  0.086419  0.000000  0.000000  0.109709  0.106350  0.060684   \n",
       "3026  0.225919  0.065666  0.076128  0.000000  0.000000  0.080811  0.184447   \n",
       "3027  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3028  0.000000  0.000000  0.064975  0.197900  0.142301  0.137943  0.000000   \n",
       "3029  0.000000  0.064889  0.075228  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "          week       pay     food  ...  mouth  toronto  honor  attack  variou  \\\n",
       "0     0.000000  0.000000  0.24076  ...    0.0      0.0    0.0     0.0     0.0   \n",
       "1     0.000000  0.000000  0.00000  ...    0.0      0.0    0.0     0.0     0.0   \n",
       "2     0.000000  0.000000  0.00000  ...    0.0      0.0    0.0     0.0     0.0   \n",
       "3     0.000000  0.000000  0.00000  ...    0.0      0.0    0.0     0.0     0.0   \n",
       "4     0.000000  0.000000  0.00000  ...    0.0      0.0    0.0     0.0     0.0   \n",
       "...        ...       ...      ...  ...    ...      ...    ...     ...     ...   \n",
       "3025  0.000000  0.000000  0.00000  ...    0.0      0.0    0.0     0.0     0.0   \n",
       "3026  0.000000  0.000000  0.00000  ...    0.0      0.0    0.0     0.0     0.0   \n",
       "3027  0.000000  0.000000  0.00000  ...    0.0      0.0    0.0     0.0     0.0   \n",
       "3028  0.160915  0.244596  0.00000  ...    0.0      0.0    0.0     0.0     0.0   \n",
       "3029  0.093153  0.000000  0.00000  ...    0.0      0.0    0.0     0.0     0.0   \n",
       "\n",
       "      event  scholarship  film   36  americorp  \n",
       "0       0.0          0.0   0.0  0.0        0.0  \n",
       "1       0.0          0.0   0.0  0.0        0.0  \n",
       "2       0.0          0.0   0.0  0.0        0.0  \n",
       "3       0.0          0.0   0.0  0.0        0.0  \n",
       "4       0.0          0.0   0.0  0.0        0.0  \n",
       "...     ...          ...   ...  ...        ...  \n",
       "3025    0.0          0.0   0.0  0.0        0.0  \n",
       "3026    0.0          0.0   0.0  0.0        0.0  \n",
       "3027    0.0          0.0   0.0  0.0        0.0  \n",
       "3028    0.0          0.0   0.0  0.0        0.0  \n",
       "3029    0.0          0.0   0.0  0.0        0.0  \n",
       "\n",
       "[3030 rows x 1500 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#New version\n",
    "#Pre-processign text with TFIDF vectorizer and SVD dimesnionlaity reduction\n",
    "\n",
    "def better_preprocessor(s):\n",
    "    \n",
    "    ### STUDENT START ###\n",
    "    s = s.lower() # to lowercase\n",
    "    \n",
    "    #Stemming and lemmatization\n",
    "    ps = PorterStemmer()\n",
    "    lemmer = WordNetLemmatizer()\n",
    "    words = word_tokenize(s)\n",
    "    cleaned_words = [lemmer.lemmatize(ps.stem(word)) for word in words]\n",
    "    \n",
    "    #Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    new_cleaned_words = [w for w in cleaned_words if not w.lower() in stop_words]\n",
    "    s = ''\n",
    "    for word in new_cleaned_words:\n",
    "        if word == '.' or word == '?' or word == ',':\n",
    "            s = s + word\n",
    "        else:\n",
    "            s = s + ' ' + word\n",
    "    \n",
    "    return s\n",
    "    ### STUDENT END ###\n",
    "\n",
    "\n",
    "def P5():\n",
    "    ### STUDENT START ###\n",
    "    \n",
    "    #Logistic Regression model (with no preprocessing of text)\n",
    "    vectorizer = CountVectorizer(preprocessor= lambda x: x)\n",
    "    X_train_counts_raw = vectorizer.fit_transform(train_data )\n",
    "    print('Size of train Vocabulary with empty pre-processor is:', X_train_counts_raw.shape)\n",
    "    \n",
    "    #Logistic Regression model (with better pre-processor) and tfidfvectorizer\n",
    "    \n",
    "    #declare Vectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer(preprocessor = better_preprocessor)\n",
    "\n",
    "    #vectorizer on train data\n",
    "    X_train_counts = tfidf_vectorizer.fit_transform(train_data)\n",
    "    print('---------- \\n Size of train Vocabulary with better preprocessor is:', X_train_counts.shape)\n",
    "    \n",
    "    #feature names\n",
    "    feature_names = tfidf_vectorizer.get_feature_names()\n",
    "    print(len(feature_names))\n",
    "    \n",
    "    #SVD for dimensionlaity reduction\n",
    "    svd = TruncatedSVD(n_components=1500,n_iter=10,random_state=42)\n",
    "    \n",
    "    \n",
    "\n",
    "    ## first reduced by SVD\n",
    "    X_svd = svd.fit_transform(X_train_counts)\n",
    "    print('---------- \\n Size of train Vocabulary with better preprocessor and SVD is:', X_svd.shape)\n",
    "    \n",
    "    #explained variance by SVD reduced dimensions\n",
    "    explained_variance = svd.explained_variance_ratio_.sum()\n",
    "    print(\"Explained variance of the SVD step: {}%\".format(int(explained_variance * 100)))\n",
    "    \n",
    "    best_features = [feature_names[i] for i in svd.components_[0].argsort()[::-1]]\n",
    "    #print(best_features[:1500])\n",
    "    \n",
    "    #print(type(X_train_counts.toarray()))\n",
    "    \n",
    "    #Convert to pandas df\n",
    "    df_orig = pd.DataFrame(X_train_counts.toarray(), columns=feature_names)\n",
    "    df = pd.DataFrame(df_orig, columns=best_features[:1500])\n",
    "    #print(df)\n",
    "\n",
    "    return (df)\n",
    "    ### STUDENT END ###\n",
    "\n",
    "P5()\n",
    "\n",
    "# Link: https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "#SVD: https://scikit-learn.org/stable/modules/decomposition.html\n",
    "#writeout featurenames: https://stackoverflow.com/questions/44633571/how-can-i-get-the-feature-names-from-sklearn-truncatedsvd-object/49055864"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train Vocabulary with empty pre-processor is: (3030, 12637)\n",
      "---------- \n",
      " Size of train Vocabulary with better preprocessor is: (3030, 8133)\n",
      "8133\n",
      "---------- \n",
      " Size of train Vocabulary with better preprocessor and SVD is: (3030, 1500)\n",
      "Explained variance of the SVD step: 90%\n"
     ]
    }
   ],
   "source": [
    "Text_features = P5()\n",
    "Text_features.to_csv('/Users/kanikamahajan/Desktop/MIDS/W207/Projects/text_processed_features.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Combine new text processed features with main dataframe\n",
    "#print(train_data.info())\n",
    "df_text_time_merge = pd.concat([train_data_raw, Text_features], axis=1, join=\"inner\")\n",
    "df_text_time_merge.to_csv('/Users/kanikamahajan/Desktop/MIDS/W207/Projects/df_text_time_merge.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTIPLE CLASSIFIERS TO SEE ACCURACY WITH ONLY TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer => transformer => classifier pipeline using pipeline class in sklearn\n",
    "#Logistic Regression\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', LogisticRegression(penalty='l2', C=0.5,solver=\"liblinear\", multi_class=\"auto\")),])\n",
    "text_clf.fit(train_data, train_labels)\n",
    "predicted = text_clf.predict(dev_data)\n",
    "np.mean(predicted == dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42,max_iter=5, tol=None)),])\n",
    "text_clf.fit(train_data, train_labels)\n",
    "predicted = text_clf.predict(dev_data)\n",
    "np.mean(predicted == dev_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRID Search for tuning hyperparameters\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "                  'tfidf__use_idf': (True, False),\n",
    "                  'clf__alpha': (1e-2, 1e-3),\n",
    "                 }\n",
    "\n",
    "gs_clf = GridSearchCV(text_clf, parameters, cv=5, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(train_data, train_labels)\n",
    "train_data[gs_clf.predict()]\n",
    "gs_clf.best_score_\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NB\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', BernoulliNB()),])\n",
    "text_clf.fit(train_data, train_labels)\n",
    "predicted = text_clf.predict(dev_data)\n",
    "np.mean(predicted == dev_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K nearest neighbors\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', KNeighborsClassifier(n_neighbors=5)),])\n",
    "text_clf.fit(train_data, train_labels)\n",
    "predicted = text_clf.predict(dev_data)\n",
    "np.mean(predicted == dev_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi layer Neural networks\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(5, 2), random_state=1)),])\n",
    "text_clf.fit(train_data, train_labels)\n",
    "predicted = text_clf.predict(dev_data)\n",
    "np.mean(predicted == dev_labels)\n",
    "\n",
    "#Link: https://scikit-learn.org/stable/modules/neural_networks_supervised.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OLD\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Extracting post text from df\n",
    "text_vector = train_data_raw.iloc[:, 6]\n",
    "\n",
    "#Using Countvectorizer to convert into bag of words\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "vectorizer.fit_transform(text_vector)\n",
    "\n",
    "\n",
    "# text into the vectorizer to get back counts\n",
    "vector = vectorizer.transform(text_vector)\n",
    "\n",
    "# Our final vector:\n",
    "print('Full vector: ')\n",
    "text_np = vector.toarray()\n",
    "\n",
    "#Convert to pandas df\n",
    "df = pd.DataFrame(text_np, columns=vectorizer.get_feature_names())\n",
    "df.head()\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Selecting features and outcome data columns \n",
    "\n",
    "cols = df_new.columns\n",
    "desired_cols = list([cols[2], cols[5], cols[10], cols[12], cols[16], cols[18], cols[20], cols[21], cols[25], cols[32]]) + list(cols[32:])\n",
    "#df_new[desired_cols].head(100)\n",
    "\n",
    "x = df_new[desired_cols] #features\n",
    "y = df_new.iloc[:, 22]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test data does not have the outcome column so splitting training data into test, dev and training\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_ratio = 0.75\n",
    "validation_ratio = 0.15\n",
    "test_ratio = 0.10\n",
    "\n",
    "# train is now 75% of the entire data set\n",
    "# the _junk suffix means that we drop that variable completely\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=1 - train_ratio, random_state=1)\n",
    "\n",
    "# test is now 10% of the initial data set\n",
    "# validation is now 15% of the initial data set\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(X_test, y_test, test_size=test_ratio/(test_ratio + validation_ratio), random_state=1) \n",
    "\n",
    "#print(X_train, X_dev, X_test)\n",
    "print(X_test.shape)\n",
    "df_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#test data does not have the outcome column so splitting training data into test and training\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K neighbors classifier\n",
    "classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_dev, y_pred))\n",
    "print(classification_report(y_dev, y_pred))\n",
    "report = classification_report(y_dev, y_pred, output_dict=True)\n",
    "print(\"Accuracy for 5 Nearest Neighbor model\", report['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "\n",
    "# import the class\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# instantiate the model (using the default parameters)\n",
    "logreg = LogisticRegression(max_iter= 1000)\n",
    "\n",
    "# fit the model with data\n",
    "logreg.fit(X_train,y_train)\n",
    "\n",
    "#\n",
    "y_pred=logreg.predict(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_dev, y_pred))\n",
    "print(classification_report(y_dev, y_pred))\n",
    "report_log = classification_report(y_dev, y_pred, output_dict=True)\n",
    "print(\"Accuracy for Logistic regression model is\", report_log['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the metrics class for printing confusion matrix\n",
    "from sklearn import metrics\n",
    "cnf_matrix = metrics.confusion_matrix(y_dev, y_pred)\n",
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing Confusion Matrix using Heatmap\n",
    "import seaborn as sns\n",
    "class_names=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##SAMPLE TEXT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "sample_text = [\"One of the most basic ways we can numerically represent words\", \n",
    "               \"is through the one-hot encoding method (also sometimes called \",\n",
    "               \"count vectorizing).\"]\n",
    "vectorizer.fit(sample_text)\n",
    "print('Vocabulary: ')\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text into the vectorizer to get back counts\n",
    "vector = vectorizer.transform(sample_text)\n",
    "\n",
    "# Our final vector:\n",
    "print('Full vector: ')\n",
    "print(vector.toarray())\n",
    "print(vector)\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "new_corpus=[' '.join([stemmer.stem(word) for word in text.split(' ')])\n",
    "          for text in sample_text]\n",
    "print(new_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
