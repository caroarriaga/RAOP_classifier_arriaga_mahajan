{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "\n",
    "Now, we will work on feature engineering to optimize our model.\n",
    "We can divide our features in two types: Meta and Text based.\n",
    "\n",
    "### Meta features\n",
    "These will include all the features that we get from the user or information we can obtain given our understanding of the english language.\n",
    "\n",
    "1. Post upvotes \n",
    "2. Days since request\n",
    "3. User subs on (count of subreddits) \n",
    "4. User activity comments\n",
    "5. User activity comments raop \n",
    "6. User posts reddit \n",
    "7. User posts raop \n",
    "8. Comment count \n",
    "9. User rate start \n",
    "10. User rate end \n",
    "11. Account age (days)\n",
    "12. Day of the week\n",
    "13. Day of the month\n",
    "14. Month\n",
    "15. Week of the year\n",
    "16. Text was edited\n",
    "\n",
    "### Text based features\n",
    "These will include all the features that are related to frequency, svd, sentiment.\n",
    "\n",
    "1. Text length (word count) \n",
    "2. Text compound sentiment \n",
    "3. Title length (word count)\n",
    "4. Title sentiment\n",
    "5. TFIDF\n",
    "6. SVD\n",
    "\n",
    "## Let's import libraries and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# Data manipulation\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ML Scikit\n",
    "\n",
    "# NLP\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "\n",
    "## SVD\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "## Decision Trees\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "# Regression model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# KNN Model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Feature handling\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Reports\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "\n",
    "# For producing decision tree diagrams.\n",
    "from IPython.core.display import Image, display\n",
    "from six import StringIO\n",
    "\n",
    "#Visualizing Confusion Matrix using Heatmap\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "df = pd.read_csv('../data/interim/logit_sentiments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4040, 19)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4040, 19)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.isnull(df['requester_received_pizza']).sum() # remove one null data point in outcome variable\n",
    "\n",
    "df = df[df['requester_received_pizza'].notna()]\n",
    "\n",
    "# Confirm no null values remaining\n",
    "pd.isnull(df['requester_received_pizza']).sum()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data to evaluate new features\n",
    "\n",
    "We're using the baseline classifier - Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Handling for Model\n",
    "\n",
    "X, Y = df.iloc[:,:-1], df.iloc[:,-1]\n",
    "\n",
    "# Shuffle the data, but make sure that the features and accompanying labels stay in sync.\n",
    "np.random.seed(0)\n",
    "shuffle = np.random.permutation(np.arange(X.shape[0]))\n",
    "X, Y = X.iloc[shuffle], Y.iloc[shuffle]\n",
    "\n",
    "# Split into train and test.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=1)\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(X_test, y_test, test_size=0.4, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train X,y -  (3030, 18)   (3030,)\n",
      "Dev X,y   -   (606, 18)    (606,)\n",
      "Test X,y  -   (404, 18)    (404,)\n",
      "Total:  4040\n"
     ]
    }
   ],
   "source": [
    "# Let's see the different tests\n",
    "\n",
    "print(\"Train X,y - \", X_train.shape, \" \", y_train.shape)\n",
    "print(\"Dev X,y   -  \", X_dev.shape, \"  \", y_dev.shape)\n",
    "print(\"Test X,y  -  \", X_test.shape, \"  \", y_test.shape)\n",
    "print(\"Total: \", X_train.shape[0] + X_dev.shape[0] +  X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_train)\n",
    "df['requester_received_pizza'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing text with TFIDF vectorizer and SVD dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see first the text we are interested in analyzing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3957    Hello RAOP,\\n\\nfigured I'd give posting here a...\n",
       "2163    Hi RAOP!\\n\\nMy two roommates and I are all ful...\n",
       "2170    I've recently graduated university. I think I ...\n",
       "2027    So, I didn't find out my wife was leaving me t...\n",
       "483     (*throwaway acct*)\\nI never thought I'd get to...\n",
       "                              ...                        \n",
       "1899    First time on Random Acts of Pizza, my wallet ...\n",
       "3082    My son has recieved his meal, thanks veritas27...\n",
       "2724    Hey, I'm a broke college student, would really...\n",
       "2669    I live in Goodyear, AZ.\\n\\nI can receive pizza...\n",
       "1992    Well, my rent is paid up until June, and [my c...\n",
       "Name: request_text, Length: 3030, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check nan\n",
    "# pd.isnull(X_train['request_text']).sum()\n",
    "# X_train['request_text'].count()\n",
    "\n",
    "# See text messages\n",
    "X_train.loc[:,'request_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fills nan in text features\n",
    "# Input: array, array name\n",
    "# Output: fill array nan with 'the'\n",
    "def fill_nan_text(texts, feature):\n",
    "    texts = texts.replace(np.nan, 'the', regex=True)\n",
    "    return texts\n",
    "\n",
    "# Improve text tokenization\n",
    "# Input: string\n",
    "# Output: string without stop words, and applied\n",
    "# lemmatization and stemming\n",
    "def better_preprocessor(s):\n",
    "    \n",
    "    s = s.lower() # to lowercase\n",
    "    \n",
    "    #Stemming and lemmatization\n",
    "    ps = PorterStemmer()\n",
    "    lemmer = WordNetLemmatizer()\n",
    "    words = word_tokenize(s)\n",
    "    cleaned_words = [lemmer.lemmatize(ps.stem(word)) for word in words]\n",
    "    \n",
    "    #Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    new_cleaned_words = [w for w in cleaned_words if not w.lower() in stop_words]\n",
    "    s = ''\n",
    "    for word in new_cleaned_words:\n",
    "        if word == '.' or word == '?' or word == ',':\n",
    "            s = s + word\n",
    "        else:\n",
    "            s = s + ' ' + word\n",
    "    \n",
    "    return s\n",
    "\n",
    "# Vectorizes the text data into separated tokens as features\n",
    "# Input: array with text\n",
    "# Output: print size of vocabulary\n",
    "def create_text_features_no_processor(train):\n",
    "    \n",
    "    # Vectorization with empty preprocessing of text\n",
    "    vectorizer = CountVectorizer(preprocessor= lambda x: x)\n",
    "    X_train_counts_raw = vectorizer.fit_transform(train)\n",
    "    print('Size of train Vocabulary with empty pre-processor is:', X_train_counts_raw.shape)\n",
    "\n",
    "# Vectorizes the text data into separated tokens as features and applies better processor\n",
    "# Input: array with text\n",
    "# Output: print size of vocabulary\n",
    "def create_text_features_with_processor(train, test):\n",
    "    \n",
    "    # Vectorization with better preprocessor\n",
    "    # declare Vectorizer with counts of words\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "                                       analyzer='word',\n",
    "                                       token_pattern=r'\\w{1,}',\n",
    "#                                        min_df=3, \n",
    "                                       preprocessor = better_preprocessor,\n",
    "                                       ngram_range=(1,3))\n",
    "\n",
    "    #vectorizer on train data\n",
    "    X_train_counts = tfidf_vectorizer.fit_transform(fill_nan_text(train,'request_text'))\n",
    "    X_test = tfidf_vectorizer.transform(fill_nan_text(test,'request_text'))\n",
    "    print('---------- \\nSize of train Vocabulary with better preprocessor is:', X_train_counts.shape[1])\n",
    "    \n",
    "    #feature names\n",
    "    feature_names = tfidf_vectorizer.get_feature_names()\n",
    "#     print(len(feature_names))\n",
    "    return tfidf_vectorizer,X_train_counts, X_test, feature_names\n",
    "\n",
    "def svd_reduction(X_train_counts,X_test,svd_comps, feature_names):\n",
    "    \n",
    "    #SVD for dimensionality reduction\n",
    "    svd = TruncatedSVD(n_components=svd_comps,n_iter=10,random_state=42)\n",
    "    normalizer = Normalizer(copy=False) #Normalizing SVD results\n",
    "    lsa = make_pipeline(svd, normalizer) #making pipeline for svd and normalizer\n",
    "\n",
    "    ## reduced by SVD\n",
    "    X_svd = lsa.fit_transform(X_train_counts)\n",
    "    X_test_svd = lsa.transform(X_test)\n",
    "    print('---------- \\nSize of train Vocabulary with better preprocessor and SVD is:', X_svd.shape[1])\n",
    "    \n",
    "    #explained variance by SVD reduced dimensions\n",
    "    explained_variance = svd.explained_variance_ratio_.sum()\n",
    "    print(\"Explained variance of the SVD step: {}%\".format(int(explained_variance * 100)))\n",
    "    \n",
    "    #extract feature names corresponding to the 1500 SVD components\n",
    "    best_features = [feature_names[i] for i in svd.components_[0].argsort()[::-1]]\n",
    "    #print(best_features[:1500])\n",
    "    \n",
    "    print(f\"Train array: {X_svd.shape}\\nTest array: {X_test_svd.shape}\")\n",
    "    \n",
    "    #Convert to pandas df\n",
    "#     df_orig = pd.DataFrame(X_train_counts.toarray(), columns=feature_names)\n",
    "#     df = pd.DataFrame(df_orig, columns=best_features[:1500])\n",
    "  \n",
    "    return X_svd, X_test_svd\n",
    "\n",
    "def clean_array(array):\n",
    "    # Remove original text variables\n",
    "    array.pop('request_title')\n",
    "    array.pop('request_text')\n",
    "    # X_dev.columns\n",
    "    return array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge meta and text data for Classification And Build Logisitc Regression and Random Forest Classifiers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- \n",
      "Size of train Vocabulary with better preprocessor is: 196211\n",
      "---------- \n",
      "Size of train Vocabulary with better preprocessor and SVD is: 200\n",
      "Explained variance of the SVD step: 13%\n",
      "Train array: (3030, 200)\n",
      "Test array: (606, 200)\n",
      "Train meta: (3030, 16) - Train Text SVD: (3030, 200)\n",
      "Test meta: (606, 16) - Test Text SVD: (606, 200)\n",
      "Test meta + text: (3030, 216) - Test meta + text: (606, 216)\n",
      "**************************\n",
      "Logistic regression\n",
      "Accuracy for Logistic regression model is 0.8449\n",
      "**************************\n",
      "Random forest classifier\n",
      "Accuracy (xg boost): 0.83 - d=2 - e=15\n",
      "Accuracy (adaboost): 0.83 - d=2 - e=15\n",
      "Accuracy (xg boost): 0.83 - d=2 - e=30\n",
      "Accuracy (xg boost): 0.83 - d=3 - e=30\n",
      "**************************\n",
      "The best classifier is xg_boost with acc=0.835 - d=3 - e=30\n"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "\n",
    "svd_components = 200 # tested with 200, 400, 600, 1000, 1500\n",
    "cs=1 # tested with 0.01,0.1,1,10,100\n",
    "depths = [2,3,4]\n",
    "estimators = [15, 30, 80]\n",
    "best=[0,0,0,0]\n",
    "\n",
    "# Data Handling for Model\n",
    "\n",
    "X, Y = df.iloc[:,:-1], df.iloc[:,-1]\n",
    "\n",
    "# Shuffle the data, but make sure that the features and accompanying labels stay in sync.\n",
    "np.random.seed(0)\n",
    "shuffle = np.random.permutation(np.arange(X.shape[0]))\n",
    "X, Y = X.iloc[shuffle], Y.iloc[shuffle]\n",
    "\n",
    "# Split into train and test.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=1)\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(X_test, y_test, test_size=0.4, random_state=1)\n",
    "\n",
    "\n",
    "# Vectorize with Tfidf + SVD \n",
    "# Vectorize with TfIDF + SVD\n",
    "count_vector, train_counts, test_counts, features = create_text_features_with_processor(\n",
    "    X_train.loc[:,'request_text'], \n",
    "    X_dev.loc[:,'request_text'])\n",
    "\n",
    "train_svd, test_svd = svd_reduction(train_counts, test_counts,\n",
    "                                    svd_components,\n",
    "                                   features)\n",
    "\n",
    "# Clean text variables: title and request\n",
    "X_train = clean_array(X_train)\n",
    "X_dev = clean_array(X_dev)\n",
    "\n",
    "print(f\"Train meta: {X_train.shape} - Train Text SVD: {train_svd.shape}\") \n",
    "print(f\"Test meta: {X_dev.shape} - Test Text SVD: {test_svd.shape}\")\n",
    "\n",
    "X_train_mt = np.concatenate((X_train,train_svd), axis=1)\n",
    "X_dev_mt = np.concatenate((X_dev,test_svd), axis=1)\n",
    "print(f\"Test meta + text: {X_train_mt.shape} - Test meta + text: {X_dev_mt.shape}\")\n",
    "\n",
    "# fit model with L2 Regularization\n",
    "print(\"**************************\")\n",
    "print(\"Logistic regression\")\n",
    "lr_model = LogisticRegression(C=cs, max_iter= 1000, solver='liblinear',\n",
    "                             multi_class=\"auto\", penalty ='l2')\n",
    "\n",
    "#print(X_train_mt.shape, y_train.shape)\n",
    "y_train_binarized = [1 if x == True else 0 for x in y_train] ### Binarize to make it an accpetable input to fit model\n",
    "y_dev_binarized = [1 if x == True else 0 for x in y_dev]\n",
    "lr_model.fit(X_train_mt, y_train_binarized)\n",
    "\n",
    "# predictions\n",
    "y_pred = lr_model.predict(X_dev_mt)\n",
    "\n",
    "# results\n",
    "#     cnf_matrix = metrics.confusion_matrix(y_dev_binarized, y_pred)\n",
    "# print(cnf_matrix)\n",
    "#     print(classification_report(y_dev_binarized, y_pred))\n",
    "report_log = classification_report(y_dev_binarized, y_pred, output_dict=True)\n",
    "print(\"Accuracy for Logistic regression model is\", round(report_log['accuracy'],4))\n",
    "print(\"**************************\")\n",
    "print(\"Random forest classifier\")\n",
    "\n",
    "for d in depths:\n",
    "    for e in estimators:\n",
    "        # XG Boost classifier\n",
    "        clf = xgb.XGBClassifier(max_depth=d, \n",
    "                                n_estimators=e, \n",
    "#                                 colsample_bytree=0.8,\n",
    "#                                 subsample=0.8, \n",
    "#                                 nthread=10, \n",
    "                                learning_rate=0.1,\n",
    "                                use_label_encoder=False,\n",
    "                                eval_metric='mlogloss')\n",
    "        clf.fit(X_train_mt, y_train_binarized)\n",
    "        predictions = clf.predict_proba(X_dev_mt)\n",
    "        if clf.score(X_dev_mt, y_dev_binarized) > best[0]:\n",
    "            print (f'Accuracy (xg boost): {clf.score(X_dev_mt, y_dev_binarized):.2} - d={d} - e={e}')\n",
    "            best[0] = clf.score(X_dev_mt, y_dev_binarized)\n",
    "            best[1] = d\n",
    "            best[2] = e\n",
    "            best[3] = 'xg_boost'\n",
    "        # Random Forest\n",
    "        rfc = RandomForestClassifier(n_estimators=e, max_depth=d)\n",
    "        rfc.fit(X_train_mt, y_train_binarized)\n",
    "        if clf.score(X_dev_mt, y_dev_binarized) > best[0]:\n",
    "            print (f'Accuracy (RF): {rfc.score(X_dev_mt, y_dev_binarized):.2} - d={d} - e={e}')\n",
    "            best[0] = rfc.score(X_dev_mt, y_dev_binarized)\n",
    "            best[1] = d\n",
    "            best[2] = e\n",
    "            best[3] = 'RF'\n",
    "            \n",
    "        # Ada Boost\n",
    "        abc = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=d)\n",
    "                                 , n_estimators=e, learning_rate=0.1)\n",
    "        abc.fit(X_train_mt, y_train_binarized)\n",
    "        if abc.score(X_dev_mt, y_dev_binarized) > best[0]:\n",
    "            print(f'Accuracy (adaboost): {abc.score(X_dev_mt, y_dev_binarized):.2} - d={d} - e={e}')\n",
    "            best[0] = rfc.score(X_dev_mt, y_dev_binarized)\n",
    "            best[1] = d\n",
    "            best[2] = e\n",
    "            best[3] = 'ada'\n",
    "\n",
    "print(\"**************************\")\n",
    "print(f\"The best classifier is {best[3]} with acc={best[0]:.3} - d={best[1]} - e={best[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Neural Network\n",
    "\n",
    "We are building the 3 layer multilayer neural network. Number of neurons in 3 layers is same as number of features in our dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8118811881188119\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(216,216,216), activation='logistic', solver='adam', max_iter=500)\n",
    "mlp.fit(X_train_mt,y_train_binarized)\n",
    "y_pred = mlp.predict(X_dev_mt)\n",
    "\n",
    "# print(confusion_matrix(y_dev_binarized,y_pred))\n",
    "# print(classification_report(y_dev_binarized,y_pred))\n",
    "\n",
    "# Model Accuracy: how often does model classify correctly\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_dev_binarized, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Neural Network\n",
    "\n",
    "Next we tried deep neural network to further improve our model accuracy. \n",
    "\n",
    "As this is a binary classification problem framed as predicting the likelihood of a data row belonging to one of the two classes, e.g. the class that you assign the integer value 1, whereas the other class is assigned the value 0. \n",
    "\n",
    "Hyperparameters used: Output Layer Configuration: One node with a sigmoid activation unit. \n",
    "Loss Function: Binary Cross-Entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 216)               46872     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 217       \n",
      "=================================================================\n",
      "Total params: 47,089\n",
      "Trainable params: 47,089\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras import backend as K\n",
    "\n",
    "K.clear_session() #reset keras session everytime we run the model\n",
    "\n",
    "#Converting y from list to numpy array to feed in the model\n",
    "y_train_binarized = np.array(y_train_binarized)\n",
    "y_dev_binarized = np.array(y_dev_binarized)\n",
    "\n",
    "# number of nodes is same as number of features\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Dense(216,activation=\"sigmoid\")) \n",
    "model.add(layers.Dense(1,activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(optimizer=\"adam\",loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "history = model.fit(X_train_mt, y_train_binarized, verbose=0, batch_size=50, epochs=100, \n",
    "                    validation_data=(X_dev_mt, y_dev_binarized))\n",
    "\n",
    "# summarize the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy last epoch\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8943894505500793"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history['acc']\n",
    "print(\"Accuracy last epoch\")\n",
    "history.history.get('acc')[-1] # printing model accuracy at last epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "After trying various machine learning classifiers our model built using dense neural networks \n",
    "achieved the highest accuracy of 0.89. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
